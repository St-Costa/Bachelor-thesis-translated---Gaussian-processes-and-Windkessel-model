\chapter{Conclusions and future directions}
This chapter summarizes the main results obtained in the course of the work, and proposes \textit{future directions} that could improve what has been studied in both theoretical and practical terms.

\begin{textblock*}{0.64\textwidth}(3.5cm+0.36\textwidth,18.5cm)
\epigraph{A conclusion is the place where you get tired of thinking.}{Arthur Bloch}
\end{textblock*}

\newpage



\section{Conclusions}
It was seen throughout the elaborate what Gaussian processes are and, in particular, explained in what sense they generalize the multivariate Gaussian distribution. The main kernel functions were illustrated, explaining the meaning of the parameters of the functions (called in supervised learning \textit{hyperparameters}). It was then explained how to generalize covariance functions in multiple dimensions, and then apply this generalization to the context of supervised learning.

Then it was explained how to use Gaussian processes for the prediction of observations without noise (or interpolation) and with noise (\textit{noisy} observations), showing how simple in theoretical terms it is to obtain remarkable results.\\

Despite what has been seen, i.e., the simplicity and power of Gaussian processes, it is worth explaining why they are not widely used in the scientific landscape: only recently, in fact, there are some research groups applying them to hemodynamic contexts, such as \cite{doi:10.1098/rsta.2019.0334} and \cite{Yuhn2022.03.10.483573}. The main reasons that \cite{rasmussen_gaussian_2006} identifies to motivate the low scientific interest are two: the first is that the application of Gaussian processes requires the handling of large matrices and in particular the inversion of them, something that has become computationally addressable only in recent decades; the second is that most of the theoretical study has been done using the same covariance functions, with little awareness about them and thus without exploiting the power of this technology. Research like \cite{duvenaud_automatic_2014} and books like \cite{rasmussen_gaussian_2006} have certainly helped the scientific community in this regard. \\

The Windkessel model was then introduced in the paper, leaving extensive coverage of the application part. Although the model is rather simple, being formed only by a differential equation, it provides results of remarkable accuracy. Within the same chapter, the local sensitivity of the variables with respect to the parameters was studied, which allowed to understand how little influence the distal pressure $P_d$ has on MAP, DBP, SBP, PP; for this reason, it was discarded from the supervised learning parameters because, leaving it, one risked worsening the performance of the statistical model both in terms of learning time and accuracy.\\

Finally, the results of supervised learning performed with the GPErks library were reported after explaining its operation from a statistical point of view. These results allow us to conclude that indeed Gaussian processes provide a shortcut to the approximation of MAP, DBP, SBP and PP compared to the use of the Windkessel model. With the right parameters, supervised learning has been seen to provide an acceptable standard deviation, hence error,\footnote{The term \textit{acceptable} refers to the precision that can be accepted in a clinical setting where measurement errors of physiological parameters can be very high.} with fairly short training. Moreover, from the input parameters of the Windkessel model, the trained Gaussian process is able to return the value of MAP, SBP, DBP and PP much faster and with less computational cost than using the Windkessel model, then solving a differential equation to convergence. For these reasons, Gaussian processes have proven to be a tool with high potential and wide uses in applied mathematics.



\section{Future directions}
The in-depth and systematic study of Gaussian processes, the Bayesian theoretical basis on which supervised learning rests and the Windkessel model make it easy to identify, in retrospect, how the approach used can be improved.\\

For example, the squared-exponential kernel and a linear mean function were used in the paper. For the purposes of what was studied they worked perfectly, but the choice of kernel function and mean function deserves to be done with proper care. As seen in \ref{section: mauna loa}, the choice of composite covariance functions may require a lengthy study starting with the knowledge (which therefore needs to be thorough) of the event to be modeled while obtaining, however, a considerable increase in accuracy in the training phase.

The early stopper should be studied extensively and possibly modified\footnote{Early stoppers do not follow precise rules; there is scientific research that suggests their form and provides examples of their implementation. Optimal would be to build them specifically for each problem, but this can be very time-consuming} based on the specific problem to best avoid overfitting and to precisely impose the number of EPOCHS and thus the execution time required at the training phase. In clinical settings, for example, a low execution time at the expense of lower accuracy may be preferable to provide real-time support to the clinician. Indeed, it should be noted that in clinical settings it is unnecessary to require high accuracy from training results because patient measurements may be subject to significant measurement error.

The choice of optimization method must be made based on the problem being addressed. In fact, each method has advantages and disadvantages that must be carefully studied in order to optimize the training. Furthermore, based on the method, it is also necessary to study in depth the parameters that can be set, for example the learning rate, which can modify the speed of training and its performance.


Another improvement that can be made to training is to use real data to verify the model, allowing for more reliable feedback on its functioning. There are several ways to do this, a simple way is to use a validation set of real data, so that during the training phase the model, after each training phase on the training set, validates its functioning on real data. This improvement is not always possible, since sometimes there are no real datasets of the values that interest the training. For example, for the problem addressed in the paper, an adequate database of real data could not be found: the only possibility was to use a database generated by a more complex model than the Windkessel model.

One training approach that would significantly decrease the risk of overfitting is $K$-fold cross validation. This technique consists of dividing the entire dataset into $k$ subsamples of equal size. The training is executed $k$ times where each time one of the $k$ subsamples is used as validation test and the other $k-1$ as training set\footnote{It is possible to use one of the $k-1$ subsamples as testing set. In reality there are different forms of $k$-fold cross validation and they differ in the use of $k$ subsamples; the basic idea is to run multiple tests by partitioning the dataset.}. Then, $k$ results are obtained, which are averaged to generate a single estimate. Changing the validation set and the training set at each training lowers the probability of overfitting.

In a context like the one seen in the paper, the local sensitivity was sufficient to exclude $P_d$ from the parameters to be taken into consideration since it proved to have little influence on the values of MAP, DBP, SBP, PP (low influence also confirmed by the results of training). In much larger situations, with many more input parameters (a real problem can have hundreds and potentially even thousands of parameters), a more in-depth and non-local analysis is that of global sensitivity analysis, which allows us to understand, in a global, what are the parameters that influence the outputs. This can lead to a significant decrease in the number of parameters to consider, speeding up training and increasing performance.

Since independent training is done for each variable MAP, DBP, SBP, PP, starting from the same input parameters, approximations of MAP, DBP, SBP and PP are obtained which are not correlated with each other, in the sense that, probably, there will be $\text{PP}\neq \text{SBP} - \text{DBP}$. Generally this is not a particularly serious problem, remembering, as previously mentioned, that in clinical situations there are large measurement errors. However, simultaneous training of all variables would avoid similar problems, even though it is a rather complicated training technique.