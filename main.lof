\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {II.1}{\ignorespaces Distribution function and probability density of a standard normal distribution \blx@tocontentsinit {0}\cite {wikiNormalDistribution}.}}{8}{figure.caption.7}%
\contentsline {figure}{\numberline {II.2}{\ignorespaces Point cloud generated by a bivariate Gaussian distribution with uncorrelated components \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{11}{figure.caption.8}%
\contentsline {figure}{\numberline {II.3}{\ignorespaces Point cloud generated by a bivariate Gaussian distribution with correlated components \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{12}{figure.caption.9}%
\contentsline {figure}{\numberline {II.4}{\ignorespaces Point cloud generated by a bivariate Gaussian distribution with strongly correlated components \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{13}{figure.caption.10}%
\contentsline {figure}{\numberline {II.5}{\ignorespaces Two different visual approaches to correlation of components of multivariate Gaussian vectors: $n=2$ \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{14}{figure.caption.11}%
\contentsline {figure}{\numberline {II.6}{\ignorespaces Visualization using segments of the correlation of components of multivariate Gaussian vectors: $n=5$ \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{15}{figure.caption.12}%
\contentsline {figure}{\numberline {II.7}{\ignorespaces Weak and strong correlation of components of multivariate Gaussian vectors visualized by segments \blx@tocontentsinit {0}\cite {damianou_gaussian_2016}.}}{16}{figure.caption.13}%
\contentsline {figure}{\numberline {II.8}{\ignorespaces Visualization by segments of the correlation of components of multivariate Gaussian vectors: $n=50$ \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{16}{figure.caption.14}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {III.1}{\ignorespaces Nonlinear regression \blx@tocontentsinit {0}\cite {turner_gaussian_2016}.}}{18}{figure.caption.15}%
\contentsline {figure}{\numberline {III.2}{\ignorespaces Nonlinear regression with Gaussian processes \blx@tocontentsinit {0}\cite {turner_gaussian_2016}.}}{19}{figure.caption.16}%
\contentsline {figure}{\numberline {III.3}{\ignorespaces Four vectors generated by a Gaussian process defined as in the example \ref {esempioProcessoGaussiano}. Code \ref {Example}.}}{23}{figure.caption.17}%
\contentsline {figure}{\numberline {III.4}{\ignorespaces Graph of $k(x,x')$ linear kernel, $\sigma _b^2=1$, $\sigma _v^2=1$, $c=-1$ and $x'=1$. Code \ref {linear kernel code}}}{25}{figure.caption.18}%
\contentsline {figure}{\numberline {III.5}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the linear kernel and $\sigma _b^2=0$, $\sigma _v^2=1$, $c=0$. Code \ref {linear sample}.}}{26}{figure.caption.19}%
\contentsline {figure}{\numberline {III.6}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the linear kernel and $\sigma _b^2=0$, $\sigma _v^2=1$, parameter $c$ is varied. Code \ref {Linear - c}.}}{27}{figure.caption.20}%
\contentsline {figure}{\numberline {III.7}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the linear kernel and $\sigma _v^2=1$, $c=0$, parameter $\sigma _b^2$ is varied. Code \ref {Linear - sigmab}.}}{27}{figure.caption.21}%
\contentsline {figure}{\numberline {III.8}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the linear kernel and $\sigma _b^2=0$, $c=0$, parameter $\sigma _v^2$ is varied. Code \ref {Linear - sigmav}.}}{28}{figure.caption.22}%
\contentsline {figure}{\numberline {III.9}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(m,k)$ where $m(x)=x^3$ e $k(x,x')$ is the linear kernel, $\sigma _b^2=1$, $\sigma _v^2=10$, $c=0$. Code \ref {linear cubedmean}.}}{28}{figure.caption.23}%
\contentsline {figure}{\numberline {III.10}{\ignorespaces Graph of $k(x,x')$ squared-exponential kernel, $\sigma ^2=1$ and $l^2=1$. Code \ref {squared-exponential}.}}{29}{figure.caption.24}%
\contentsline {figure}{\numberline {III.11}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the squared-exponential kernel and $l^2=1$, $\sigma ^2=1$. Code \ref {RBF sample}.}}{30}{figure.caption.25}%
\contentsline {figure}{\numberline {III.12}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the squared-exponential kernel and $l^2=1$, parameter $\sigma ^2$ is varied. Code \ref {RBF - sigma}.}}{31}{figure.caption.26}%
\contentsline {figure}{\numberline {III.13}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the squared-exponential kernel and $\sigma ^2=1$, parameter $l^2$ is varied. Code \ref {codice9}.}}{31}{figure.caption.27}%
\contentsline {figure}{\numberline {III.14}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(m,k)$ where $m(x)=x^3$ e $k(x,x')$ is the squared-exponential kernel, $\sigma ^2=7$ and $l=0.3$. Code \ref {codice10}.}}{32}{figure.caption.28}%
\contentsline {figure}{\numberline {III.15}{\ignorespaces Graph of $k(x,x')$ periodic kernel, $\sigma ^2=1$, $l^2=1$, $p=2$. Code \ref {periodic Kernel}.}}{33}{figure.caption.29}%
\contentsline {figure}{\numberline {III.16}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the periodic kernel and $\sigma ^2=1$, $l^2=2$, $p=1$. Code \ref {periodic sample}.}}{34}{figure.caption.30}%
\contentsline {figure}{\numberline {III.17}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the periodic kernel and $l^2=1, p=1$, parameter $\sigma ^2$ is varied. Code \ref {Periodic sigma}.}}{34}{figure.caption.31}%
\contentsline {figure}{\numberline {III.18}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the periodic kernel and $\sigma ^2=1$, $l^2=1$ and parameter $p$ is varied. Code \ref {periodic p}.}}{35}{figure.caption.32}%
\contentsline {figure}{\numberline {III.19}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the periodic kernel and $\sigma ^2=1$, $p=1$ and parameter $l^2$ is varied. Code \ref {periodic l}.}}{35}{figure.caption.33}%
\contentsline {figure}{\numberline {III.20}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(m,k)$ where $m(x)=x^3$ and $k(x,x')$ the periodic kernel, $\sigma ^2=8$, $l^2=1$ and $p=0.5$. Code \ref {priodic cubedmean}.}}{36}{figure.caption.34}%
\contentsline {figure}{\numberline {III.21}{\ignorespaces Graph of function with distribution $f\sim \mathcal {GP}(0,k)$ with $k(x,x')$ the squared-exponential kernel in two dimensions in which $M=\text {diag}(1,3)^{-2}$. The function tends to change faster along the $x_1$ direction than along the $x_2$ direction. \blx@tocontentsinit {0}\cite {murphy_machine_2012}}}{37}{figure.caption.35}%
\contentsline {figure}{\numberline {III.22}{\ignorespaces Graph of $k(x,x')$ squared-exponential kernel summed to periodic kernel. $\sigma ^2=1$, $l=2$, $p=1$. Code \ref {RBF + periodic kernel}.}}{39}{figure.caption.37}%
\contentsline {figure}{\numberline {III.23}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the squared-exponential summed to periodic kernel and $l^2=1.1$, $\sigma ^2=1$, $p=1.1$. Code \ref {RBF + periodic sample}.}}{39}{figure.caption.38}%
\contentsline {figure}{\numberline {III.24}{\ignorespaces Graph of $k(x,x')$ linear kernel multiplied by linear kernel. $\sigma _b^2=0$, $\sigma _v^2=1$, $c=0$, $x'=1$. Code \ref {linear x linear}.}}{40}{figure.caption.40}%
\contentsline {figure}{\numberline {III.25}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the linear kernel multiplied by linear kernel and $\sigma _b^2=0$, $\sigma _v^2=1$, $c=0$. Code \ref {linear x linear sample}.}}{40}{figure.caption.41}%
\contentsline {figure}{\numberline {III.26}{\ignorespaces 545 observations of monthly averages of the atmospheric concentration of $CO_2$ between 1958 and 2003, the $95\%$ confidence region for a 20-year Gaussian process regression model in the future is also shown. \blx@tocontentsinit {0}\cite {rasmussen_gaussian_2006}}}{41}{figure.caption.43}%
\contentsline {figure}{\numberline {III.27}{\ignorespaces Comparison of the prediction of $CO_2$ concentration with actual data until May 2022.}}{42}{figure.caption.44}%
\contentsline {figure}{\numberline {III.28}{\ignorespaces Comparison of the prediction of $CO_2$ concentration with actual data up to 1995 to May 2022.}}{43}{figure.caption.45}%
\contentsline {figure}{\numberline {III.29}{\ignorespaces Graphical explanation of how a priori knowledge is incorporated \blx@tocontentsinit {0}\cite {gortler_visual_2019}.}}{44}{figure.caption.46}%
\contentsline {figure}{\numberline {III.30}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(\cdot ,\cdot )$ is the squared-exponential kernel; the Gaussian process was conditioned to interpolate six points. Shown in red is the function from which the points to be interpolated were chosen, in blue the mean of the conditioned Gaussian process, as dashed lines some samples of the Gaussian process. Code \ref {interpolation code}.}}{46}{figure.caption.47}%
\contentsline {figure}{\numberline {III.31}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(\cdot ,\cdot )$ is the squared-exponential kernel; the Gaussian process was conditioned to interpolate six points. Shown in blue is the 95\% confidence region, in blue the mean of the conditional Gaussian process, and as dashed lines some samples. Code \ref {interpolation confidence region code}.}}{47}{figure.caption.48}%
\contentsline {figure}{\numberline {III.32}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(\cdot ,\cdot )$ is the squared-exponential kernel; the Gaussian process was conditioned to predict a function from its noisy observations. Shown in red is the function to be predicted, in red the observed points of the function with bars representing the noise, in blue the average of the conditioned Gaussian process, as dashed lines some samples of the Gaussian process. Code \ref {Noise code}.}}{48}{figure.caption.49}%
\contentsline {figure}{\numberline {III.33}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(\cdot ,\cdot )$ is the squared-exponential kernel; the Gaussian process was conditioned to predict a function from its noisy observations. Shown in blue is the 95\% confidence region, in red with error bars the observations, in blue the mean of the conditioned Gaussian process, and as dashed lines some samples. Code \ref {Noise confidence region code}.}}{49}{figure.caption.50}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {IV.1}{\ignorespaces Esempio di overfitting. I dati (approssimativamente lineari) sono approssimati da una funzione lineare e da una polinomiale. Anche se la funzione polinomiale fornisce un adattamento quasi perfetto, ci si può aspettare che la funzione lineare generalizzi meglio i dati. \blx@tocontentsinit {0}\cite {wiki:overfitting}}}{53}{figure.caption.51}%
\contentsline {figure}{\numberline {IV.2}{\ignorespaces Overfitting nell'apprendimento supervisionato. Il training error (errore sul training set) è mostrato in blu, il validation error (errore su validation set) in rosso, entrambi in funzione del numero di cicli di training. \blx@tocontentsinit {0}\cite {wiki:overfitting}}}{54}{figure.caption.52}%
\contentsline {figure}{\numberline {IV.3}{\ignorespaces Illustrazione del metodo del gradiente su degli insiemi di livello, ad ogni iterazione viene aggiornato il learning rate.\blx@tocontentsinit {0}\cite {wiki:gradientDescend}}}{63}{figure.caption.53}%
\contentsline {figure}{\numberline {IV.4}{\ignorespaces Metodo dei momenti applicato al metodo stocastico del gradiente. \blx@tocontentsinit {0}\cite {ruder_2022}}}{65}{figure.caption.54}%
\contentsline {figure}{\numberline {IV.5}{\ignorespaces Comparazione di metodi di minimizzazione di una cost function in una rete neurale. \blx@tocontentsinit {0}\cite {kingma_adam_2017}}}{68}{figure.caption.55}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {V.1}{\ignorespaces Pressione sistemica e flusso aortico misurati in un paziente. Codice \ref {datiReali}.}}{70}{figure.caption.56}%
\contentsline {figure}{\numberline {V.2}{\ignorespaces Grafico della pressione reale e output del modello semplice. Codice \ref {modelloSemplice}.}}{71}{figure.caption.57}%
\contentsline {figure}{\numberline {V.3}{\ignorespaces Anatomia del cuore umano \blx@tocontentsinit {0}\cite {wiki:cicloCardiaco}.}}{72}{figure.caption.58}%
\contentsline {figure}{\numberline {V.4}{\ignorespaces Diagrammi che riassumono la sistole e la diastole di un cuore umano \blx@tocontentsinit {0}\cite {wiki:cicloCardiaco}.}}{73}{figure.caption.59}%
\contentsline {figure}{\numberline {V.5}{\ignorespaces Esempio di diagramma di Wiggers \blx@tocontentsinit {0}\cite {wiki:DiagrammaWiggers}.}}{75}{figure.caption.61}%
\contentsline {figure}{\numberline {V.6}{\ignorespaces Illustrazione dell'analogia sull'effetto windkessel \blx@tocontentsinit {0}\cite {wiki:WindkesselEffect}.}}{77}{figure.caption.66}%
\contentsline {figure}{\numberline {V.7}{\ignorespaces Illustrazione dell'effetto windkessel \blx@tocontentsinit {0}\cite {AaronsonPhilipI.PhilipIrving2020Tcsa}.}}{78}{figure.caption.67}%
\contentsline {figure}{\numberline {V.8}{\ignorespaces Forma circuitale del modello Windkessel a due elementi.}}{81}{figure.caption.68}%
\contentsline {figure}{\numberline {V.9}{\ignorespaces Grafico di $f_C$. Codice \ref {plotfC-code}.}}{82}{figure.caption.69}%
\contentsline {figure}{\numberline {V.10}{\ignorespaces Grafico della soluzione approssimata dell'equazione (\ref {equation}) con $C$ stimata. Codice \ref {plotSoluzioneCstimata}.}}{82}{figure.caption.70}%
\contentsline {figure}{\numberline {V.11}{\ignorespaces Grafico della soluzione approssimata dell'equazione (\ref {equation}) con $C=2,11579mL/mmHg$ e $\alpha =0,97134$. Codice \ref {soluzioneCalphastimate}.}}{83}{figure.caption.71}%
\contentsline {figure}{\numberline {V.12}{\ignorespaces Grafico della soluzione dell'equazione (\ref {equation}) approssimata dopo venti cicli cardiaci con $C=2,03424mL/mmHg$ e $\alpha =0,97354$.}}{85}{figure.caption.72}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {VI.1}{\ignorespaces Distribuzione dei dati nel database.}}{94}{figure.caption.77}%
\contentsline {figure}{\numberline {VI.2}{\ignorespaces MAP: andamento del training e validation loss, early stopper, R2Score e MSE.}}{95}{figure.caption.79}%
\contentsline {figure}{\numberline {VI.3}{\ignorespaces MAP: predizioni sui dati di input.}}{96}{figure.caption.81}%
\contentsline {figure}{\numberline {VI.4}{\ignorespaces Dipendenza di MAP da $C$ sull'intervallo di training e due intervalli attigui.}}{97}{figure.caption.83}%
\contentsline {figure}{\numberline {VI.5}{\ignorespaces Dipendenza di MAP da $C$ sull'intervallo di training.}}{97}{figure.caption.84}%
\contentsline {figure}{\numberline {VI.6}{\ignorespaces Dipendenza di MAP da $C$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{98}{figure.caption.85}%
\contentsline {figure}{\numberline {VI.7}{\ignorespaces Dipendenza di MAP da $C$ sull'intervallo attiguo a destra dell'intervallo di training.}}{98}{figure.caption.86}%
\contentsline {figure}{\numberline {VI.8}{\ignorespaces Dipendenza di MAP da $R1$ sull'intervallo di training e due intervalli attigui.}}{99}{figure.caption.88}%
\contentsline {figure}{\numberline {VI.9}{\ignorespaces Dipendenza di MAP da $R1$ sull'intervallo di training.}}{99}{figure.caption.89}%
\contentsline {figure}{\numberline {VI.10}{\ignorespaces Dipendenza di MAP da $R1$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{100}{figure.caption.90}%
\contentsline {figure}{\numberline {VI.11}{\ignorespaces Dipendenza di MAP da $R1$ sull'intervallo attiguo a destra dell'intervallo di training.}}{100}{figure.caption.91}%
\contentsline {figure}{\numberline {VI.12}{\ignorespaces Dipendenza di MAP da $R2$ sull'intervallo di training e due intervalli attigui.}}{101}{figure.caption.93}%
\contentsline {figure}{\numberline {VI.13}{\ignorespaces Dipendenza di MAP da $R2$ sull'intervallo di training.}}{101}{figure.caption.94}%
\contentsline {figure}{\numberline {VI.14}{\ignorespaces Dipendenza di MAP da $R2$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{102}{figure.caption.95}%
\contentsline {figure}{\numberline {VI.15}{\ignorespaces Dipendenza di MAP da $R2$ sull'intervallo attiguo a destra dell'intervallo di training.}}{102}{figure.caption.96}%
\contentsline {figure}{\numberline {VI.16}{\ignorespaces DBP: andamento del training e validation loss, early stopper, R2Score e MSE.}}{103}{figure.caption.98}%
\contentsline {figure}{\numberline {VI.17}{\ignorespaces DBP: predizioni sui dati di input.}}{103}{figure.caption.100}%
\contentsline {figure}{\numberline {VI.18}{\ignorespaces Dipendenza di DBP da $C$ sull'intervallo di training e due intervalli attigui.}}{104}{figure.caption.102}%
\contentsline {figure}{\numberline {VI.19}{\ignorespaces Dipendenza di DBP da $C$ sull'intervallo di training.}}{104}{figure.caption.103}%
\contentsline {figure}{\numberline {VI.20}{\ignorespaces Dipendenza di DBP da $C$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{105}{figure.caption.104}%
\contentsline {figure}{\numberline {VI.21}{\ignorespaces Dipendenza di DBP da $C$ sull'intervallo attiguo a destra dell'intervallo di training.}}{105}{figure.caption.105}%
\contentsline {figure}{\numberline {VI.22}{\ignorespaces Dipendenza di DBP da $R_1$ sull'intervallo di training e due intervalli attigui.}}{106}{figure.caption.107}%
\contentsline {figure}{\numberline {VI.23}{\ignorespaces Dipendenza di DBP da $R_1$ sull'intervallo di training.}}{106}{figure.caption.108}%
\contentsline {figure}{\numberline {VI.24}{\ignorespaces Dipendenza di DBP da $R_1$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{107}{figure.caption.109}%
\contentsline {figure}{\numberline {VI.25}{\ignorespaces Dipendenza di DBP da $R_1$ sull'intervallo attiguo a destra dell'intervallo di training.}}{107}{figure.caption.110}%
\contentsline {figure}{\numberline {VI.26}{\ignorespaces Dipendenza di DBP da $R_2$ sull'intervallo di training e due intervalli attigui.}}{108}{figure.caption.112}%
\contentsline {figure}{\numberline {VI.27}{\ignorespaces Dipendenza di DBP da $R_2$ sull'intervallo di training.}}{108}{figure.caption.113}%
\contentsline {figure}{\numberline {VI.28}{\ignorespaces Dipendenza di DBP da $R_2$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{109}{figure.caption.114}%
\contentsline {figure}{\numberline {VI.29}{\ignorespaces Dipendenza di DBP da $R_2$ sull'intervallo attiguo a destra dell'intervallo di training.}}{109}{figure.caption.115}%
\contentsline {figure}{\numberline {VI.30}{\ignorespaces PP: andamento del training e validation loss, early stopper, R2Score e MSE.}}{110}{figure.caption.117}%
\contentsline {figure}{\numberline {VI.31}{\ignorespaces PP: predizioni sui dati di input.}}{110}{figure.caption.119}%
\contentsline {figure}{\numberline {VI.32}{\ignorespaces Dipendenza di PP da $C$ sull'intervallo di training e due intervalli attigui.}}{111}{figure.caption.121}%
\contentsline {figure}{\numberline {VI.33}{\ignorespaces Dipendenza di PP da $C$ sull'intervallo di training.}}{111}{figure.caption.122}%
\contentsline {figure}{\numberline {VI.34}{\ignorespaces Dipendenza di PP da $C$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{112}{figure.caption.123}%
\contentsline {figure}{\numberline {VI.35}{\ignorespaces Dipendenza di PP da $C$ sull'intervallo attiguo a destra dell'intervallo di training.}}{112}{figure.caption.124}%
\contentsline {figure}{\numberline {VI.36}{\ignorespaces Dipendenza di PP da $R1$ sull'intervallo di training e due intervalli attigui.}}{113}{figure.caption.126}%
\contentsline {figure}{\numberline {VI.37}{\ignorespaces Dipendenza di PP da $R1$ sull'intervallo di training.}}{113}{figure.caption.127}%
\contentsline {figure}{\numberline {VI.38}{\ignorespaces Dipendenza di PP da $R1$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{114}{figure.caption.128}%
\contentsline {figure}{\numberline {VI.39}{\ignorespaces Dipendenza di PP da $R1$ sull'intervallo attiguo a destra dell'intervallo di training.}}{114}{figure.caption.129}%
\contentsline {figure}{\numberline {VI.40}{\ignorespaces Dipendenza di PP da $R2$ sull'intervallo di training e due intervalli attigui.}}{115}{figure.caption.131}%
\contentsline {figure}{\numberline {VI.41}{\ignorespaces Dipendenza di PP da $R2$ sull'intervallo di training.}}{115}{figure.caption.132}%
\contentsline {figure}{\numberline {VI.42}{\ignorespaces Dipendenza di PP da $R2$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{116}{figure.caption.133}%
\contentsline {figure}{\numberline {VI.43}{\ignorespaces Dipendenza di PP da $R2$ sull'intervallo attiguo a destra dell'intervallo di training.}}{116}{figure.caption.134}%
\contentsline {figure}{\numberline {VI.44}{\ignorespaces SBP: andamento del training e validation loss, early stopper, R2Score e MSE.}}{117}{figure.caption.136}%
\contentsline {figure}{\numberline {VI.45}{\ignorespaces SBP: predizioni sui dati di input.}}{117}{figure.caption.138}%
\contentsline {figure}{\numberline {VI.46}{\ignorespaces Dipendenza di SBP da $C$ sull'intervallo di training e due intervalli attigui.}}{118}{figure.caption.140}%
\contentsline {figure}{\numberline {VI.47}{\ignorespaces Dipendenza di SBP da $C$ sull'intervallo di training.}}{118}{figure.caption.141}%
\contentsline {figure}{\numberline {VI.48}{\ignorespaces Dipendenza di SBP da $C$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{119}{figure.caption.142}%
\contentsline {figure}{\numberline {VI.49}{\ignorespaces Dipendenza di SBP da $C$ sull'intervallo attiguo a destra dell'intervallo di training.}}{119}{figure.caption.143}%
\contentsline {figure}{\numberline {VI.50}{\ignorespaces Dipendenza di SBP da $R1$ sull'intervallo di training e due intervalli attigui.}}{120}{figure.caption.145}%
\contentsline {figure}{\numberline {VI.51}{\ignorespaces Dipendenza di SBP da $R1$ sull'intervallo di training.}}{120}{figure.caption.146}%
\contentsline {figure}{\numberline {VI.52}{\ignorespaces Dipendenza di SBP da $R1$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{121}{figure.caption.147}%
\contentsline {figure}{\numberline {VI.53}{\ignorespaces Dipendenza di SBP da $R1$ sull'intervallo attiguo a destra dell'intervallo di training.}}{121}{figure.caption.148}%
\contentsline {figure}{\numberline {VI.54}{\ignorespaces Dipendenza di SBP da $R2$ sull'intervallo di training e due intervalli attigui.}}{122}{figure.caption.150}%
\contentsline {figure}{\numberline {VI.55}{\ignorespaces Dipendenza di SBP da $R2$ sull'intervallo di training.}}{122}{figure.caption.151}%
\contentsline {figure}{\numberline {VI.56}{\ignorespaces Dipendenza di SBP da $R2$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{123}{figure.caption.152}%
\contentsline {figure}{\numberline {VI.57}{\ignorespaces Dipendenza di SBP da $R2$ sull'intervallo attiguo a destra dell'intervallo di training.}}{123}{figure.caption.153}%
\addvspace {10\p@ }
