\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {II.1}{\ignorespaces Funzione di ripartizione e densità di probabilità di una distribuzione normale standard \blx@tocontentsinit {0}\cite {wikiNormalDistribution}.}}{8}{figure.caption.7}%
\contentsline {figure}{\numberline {II.2}{\ignorespaces Nube di punti generata da una distribuzione gaussiana bivariata con componenti incorrelate \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{11}{figure.caption.8}%
\contentsline {figure}{\numberline {II.3}{\ignorespaces Nube di punti generata da una distribuzione gaussiana bivariata con componenti correlate \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{12}{figure.caption.9}%
\contentsline {figure}{\numberline {II.4}{\ignorespaces Nube di punti generata da una distribuzione gaussiana bivariata con componenti fortemente correlate \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{13}{figure.caption.10}%
\contentsline {figure}{\numberline {II.5}{\ignorespaces Due diversi approcci visivi alla correlazione di componenti di vettori gaussiani multivariati: $n=2$ \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{14}{figure.caption.11}%
\contentsline {figure}{\numberline {II.6}{\ignorespaces Visualizzazione tramite segmenti della correlazione di componenti di vettori gaussiani multivariati: $n=5$ \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{15}{figure.caption.12}%
\contentsline {figure}{\numberline {II.7}{\ignorespaces Debole e forte correlazione delle componenti di vettori gaussiani multivariati visualizzata tramite segmenti \blx@tocontentsinit {0}\cite {damianou_gaussian_2016}.}}{16}{figure.caption.13}%
\contentsline {figure}{\numberline {II.8}{\ignorespaces Visualizzazione tramite segmenti della correlazione di componenti di vettori gaussiani multivariati: $n=50$ \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{16}{figure.caption.14}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {III.1}{\ignorespaces Regressione nonlineare \blx@tocontentsinit {0}\cite {turner_gaussian_2016}.}}{18}{figure.caption.15}%
\contentsline {figure}{\numberline {III.2}{\ignorespaces Regressione nonlineare con processi gaussiani \blx@tocontentsinit {0}\cite {turner_gaussian_2016}.}}{19}{figure.caption.16}%
\contentsline {figure}{\numberline {III.3}{\ignorespaces Quattro vettori generati da un processo gaussiano definito come nell'esempio \ref {esempioProcessoGaussiano}. Codice \ref {Example}.}}{23}{figure.caption.17}%
\contentsline {figure}{\numberline {III.4}{\ignorespaces Grafico di $k(x,x')$ linear kernel, $\sigma _b^2=1$, $\sigma _v^2=1$, $c=-1$ e $x'=1$. Codice \ref {linear kernel code}}}{25}{figure.caption.18}%
\contentsline {figure}{\numberline {III.5}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è il linear kernel e $\sigma _b^2=0$, $\sigma _v^2=1$, $c=0$. Codice \ref {linear sample}.}}{26}{figure.caption.19}%
\contentsline {figure}{\numberline {III.6}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è il linear kernel e $\sigma _b^2=0$, $\sigma _v^2=1$, il parametro $c$ viene variato. Codice \ref {Linear - c}.}}{27}{figure.caption.20}%
\contentsline {figure}{\numberline {III.7}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è il linear kernel e $\sigma _v^2=1$, $c=0$, il parametro $\sigma _b^2$ viene variato. Codice \ref {Linear - sigmab}.}}{27}{figure.caption.21}%
\contentsline {figure}{\numberline {III.8}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è il linear kernel e $\sigma _b^2=0$, $c=0$, il parametro $\sigma _v^2$ viene variato. Codice \ref {Linear - sigmav}.}}{28}{figure.caption.22}%
\contentsline {figure}{\numberline {III.9}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(m,k)$ dove $m(x)=x^3$ e $k(x,x')$ è il linear kernel, $\sigma _b^2=1$, $\sigma _v^2=10$, $c=0$. Codice \ref {linear cubedmean}.}}{28}{figure.caption.23}%
\contentsline {figure}{\numberline {III.10}{\ignorespaces Grafico di $k(x,x')$ squared-exponential kernel, $\sigma ^2=1$ e $l^2=1$. Codice \ref {squared-exponential}.}}{29}{figure.caption.24}%
\contentsline {figure}{\numberline {III.11}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è lo squared-exponential kernel e $l^2=1$, $\sigma ^2=1$. Codice \ref {RBF sample}.}}{30}{figure.caption.25}%
\contentsline {figure}{\numberline {III.12}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è lo squared-exponential kernel e $l^2=1$, il parametro $\sigma ^2$ viene variato. Codice \ref {RBF - sigma}.}}{31}{figure.caption.26}%
\contentsline {figure}{\numberline {III.13}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è lo squared-exponential kernel e $\sigma ^2=1$, il parametro $l^2$ viene variato. Codice \ref {codice9}.}}{31}{figure.caption.27}%
\contentsline {figure}{\numberline {III.14}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(m,k)$ dove $m(x)=x^3$ e $k(x,x')$ lo squared-exponential kernel, $\sigma ^2=7$ e $l=0.3$. Codice \ref {codice10}.}}{32}{figure.caption.28}%
\contentsline {figure}{\numberline {III.15}{\ignorespaces Grafico di $k(x,x')$ periodic kernel, $\sigma ^2=1$, $l^2=1$, $p=2$. Codice \ref {periodic Kernel}.}}{33}{figure.caption.29}%
\contentsline {figure}{\numberline {III.16}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è il periodic kernel e $\sigma ^2=1$, $l^2=2$, $p=1$. Codice \ref {periodic sample}.}}{34}{figure.caption.30}%
\contentsline {figure}{\numberline {III.17}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è il periodic kernel e $l^2=1, p=1$, il parametro $\sigma ^2$ viene variato. Codice \ref {Periodic sigma}.}}{34}{figure.caption.31}%
\contentsline {figure}{\numberline {III.18}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è il periodic kernel e $\sigma ^2=1$, $l^2=1$ e il parametro $p$ viene variato. Codice \ref {periodic p}.}}{35}{figure.caption.32}%
\contentsline {figure}{\numberline {III.19}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è il periodic kernel e $\sigma ^2=1$, $p=1$ e il parametro $l^2$ viene variato. Codice \ref {periodic l}.}}{35}{figure.caption.33}%
\contentsline {figure}{\numberline {III.20}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(m,k)$ dove $m(x)=x^3$ e $k(x,x')$ il periodic kernel, $\sigma ^2=8$, $l^2=1$ e $p=0.5$. Codice \ref {priodic cubedmean}.}}{36}{figure.caption.34}%
\contentsline {figure}{\numberline {III.21}{\ignorespaces Grafico di funzione con distribuzione $f\sim \mathcal {GP}(0,k)$ con $k(x,x')$ lo squared-exponential kernel in due dimensioni in cui $M=\text {diag}(1,3)^{-2}$. La funzione tende a cambiare più velocemente lungo la direzione $x_1$ che lungo la direzione $x_2$. \blx@tocontentsinit {0}\cite {murphy_machine_2012}}}{37}{figure.caption.35}%
\contentsline {figure}{\numberline {III.22}{\ignorespaces Grafico di $k(x,x')$ squared-exponential kernel sommato a periodic kernel. $\sigma ^2=1$, $l=2$, $p=1$. Codice \ref {RBF + periodic kernel}.}}{39}{figure.caption.37}%
\contentsline {figure}{\numberline {III.23}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è lo squared-exponential sommato al periodic kernel e $l^2=1.1$, $\sigma ^2=1$, $p=1.1$. Codice \ref {RBF + periodic sample}.}}{39}{figure.caption.38}%
\contentsline {figure}{\numberline {III.24}{\ignorespaces Grafico di $k(x,x')$ linear kernel moltiplicato a linear kernel. $\sigma _b^2=0$, $\sigma _v^2=1$, $c=0$, $x'=1$. Codice \ref {linear x linear}.}}{40}{figure.caption.40}%
\contentsline {figure}{\numberline {III.25}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(x,x')$ è il linear kernel moltiplicato al linear kernel e $\sigma _b^2=0$, $\sigma _v^2=1$, $c=0$. Codice \ref {linear x linear sample}.}}{40}{figure.caption.41}%
\contentsline {figure}{\numberline {III.26}{\ignorespaces 545 osservazioni delle medie mensili della concentrazione atmosferica di $CO_2$ tra il 1958 e il 2003, inoltre viene mostrata la regione di confidenza del $95\%$ per un modello di regressione di processo gaussiano a 20 anni nel futuro. \blx@tocontentsinit {0}\cite {rasmussen_gaussian_2006}}}{41}{figure.caption.43}%
\contentsline {figure}{\numberline {III.27}{\ignorespaces Comparazione della predizione della concentrazione di $CO_2$ con i dati reali fino a maggio 2022.}}{42}{figure.caption.44}%
\contentsline {figure}{\numberline {III.28}{\ignorespaces Comparazione della predizione della concentrazione di $CO_2$ con i dati reali fino dal 1995 a maggio 2022.}}{43}{figure.caption.45}%
\contentsline {figure}{\numberline {III.29}{\ignorespaces Spiegazione grafica di come viene incorporata la conoscenza a priori \blx@tocontentsinit {0}\cite {gortler_visual_2019}.}}{44}{figure.caption.46}%
\contentsline {figure}{\numberline {III.30}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(\cdot ,\cdot )$ è il squared-exponential kernel; il processo gaussiano è stato condizionato per interpolare sei punti. Viene mostrata in rosso la funzione da cui sono stati scelti i punti da interpolare, in blu la media del processo gaussiano condizionato, come linee tratteggiate alcuni sample del processo gaussiano. Codice \ref {interpolation code}.}}{46}{figure.caption.47}%
\contentsline {figure}{\numberline {III.31}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(\cdot ,\cdot )$ è il squared-exponential kernel; il processo gaussiano è stato condizionato per interpolare sei punti. Viene mostrata in blu la regione di confidenza al 95\%, in blu la media del processo gaussiano condizionato e come linee tratteggiate alcuni sample. Codice \ref {interpolation confidence region code}.}}{47}{figure.caption.48}%
\contentsline {figure}{\numberline {III.32}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(\cdot ,\cdot )$ è il squared-exponential kernel; il processo gaussiano è stato condizionato per predire una funzione a partire dalle sue osservazioni rumorose. Viene mostrata in rosso la funzione da predire, in rosso i punti osservati della funzione con le barre rappresentanti il rumore, in blu la media del processo gaussiano condizionato, come linee tratteggiate alcuni sample del processo gaussiano. Codice \ref {Noise code}.}}{48}{figure.caption.49}%
\contentsline {figure}{\numberline {III.33}{\ignorespaces Grafico di funzioni con distribuzione $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(\cdot ,\cdot )$ è il squared-exponential kernel; il processo gaussiano è stato condizionato per predire una funzione a partire dalle sue osservazioni rumorose. Viene mostrata in blu la regione di confidenza al 95\%, in rosso con le barre di errore le osservazioni, in blu la media del processo gaussiano condizionato e come linee tratteggiate alcuni sample. Codice \ref {Noise confidence region code}.}}{49}{figure.caption.50}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {IV.1}{\ignorespaces Esempio di overfitting. I dati (approssimativamente lineari) sono approssimati da una funzione lineare e da una polinomiale. Anche se la funzione polinomiale fornisce un adattamento quasi perfetto, ci si può aspettare che la funzione lineare generalizzi meglio i dati. \blx@tocontentsinit {0}\cite {wiki:overfitting}}}{53}{figure.caption.51}%
\contentsline {figure}{\numberline {IV.2}{\ignorespaces Overfitting nell'apprendimento supervisionato. Il training error (errore sul training set) è mostrato in blu, il validation error (errore su validation set) in rosso, entrambi in funzione del numero di cicli di training. \blx@tocontentsinit {0}\cite {wiki:overfitting}}}{54}{figure.caption.52}%
\contentsline {figure}{\numberline {IV.3}{\ignorespaces Illustrazione del metodo del gradiente su degli insiemi di livello, ad ogni iterazione viene aggiornato il learning rate.\blx@tocontentsinit {0}\cite {wiki:gradientDescend}}}{63}{figure.caption.53}%
\contentsline {figure}{\numberline {IV.4}{\ignorespaces Metodo dei momenti applicato al metodo stocastico del gradiente. \blx@tocontentsinit {0}\cite {ruder_2022}}}{65}{figure.caption.54}%
\contentsline {figure}{\numberline {IV.5}{\ignorespaces Comparazione di metodi di minimizzazione di una cost function in una rete neurale. \blx@tocontentsinit {0}\cite {kingma_adam_2017}}}{68}{figure.caption.55}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {V.1}{\ignorespaces Pressione sistemica e flusso aortico misurati in un paziente. Codice \ref {datiReali}.}}{70}{figure.caption.56}%
\contentsline {figure}{\numberline {V.2}{\ignorespaces Grafico della pressione reale e output del modello semplice. Codice \ref {modelloSemplice}.}}{71}{figure.caption.57}%
\contentsline {figure}{\numberline {V.3}{\ignorespaces Anatomia del cuore umano \blx@tocontentsinit {0}\cite {wiki:cicloCardiaco}.}}{72}{figure.caption.58}%
\contentsline {figure}{\numberline {V.4}{\ignorespaces Diagrammi che riassumono la sistole e la diastole di un cuore umano \blx@tocontentsinit {0}\cite {wiki:cicloCardiaco}.}}{73}{figure.caption.59}%
\contentsline {figure}{\numberline {V.5}{\ignorespaces Esempio di diagramma di Wiggers \blx@tocontentsinit {0}\cite {wiki:DiagrammaWiggers}.}}{75}{figure.caption.61}%
\contentsline {figure}{\numberline {V.6}{\ignorespaces Illustrazione dell'analogia sull'effetto windkessel \blx@tocontentsinit {0}\cite {wiki:WindkesselEffect}.}}{77}{figure.caption.66}%
\contentsline {figure}{\numberline {V.7}{\ignorespaces Illustrazione dell'effetto windkessel \blx@tocontentsinit {0}\cite {AaronsonPhilipI.PhilipIrving2020Tcsa}.}}{78}{figure.caption.67}%
\contentsline {figure}{\numberline {V.8}{\ignorespaces Forma circuitale del modello Windkessel a due elementi.}}{81}{figure.caption.68}%
\contentsline {figure}{\numberline {V.9}{\ignorespaces Grafico di $f_C$. Codice \ref {plotfC-code}.}}{82}{figure.caption.69}%
\contentsline {figure}{\numberline {V.10}{\ignorespaces Grafico della soluzione approssimata dell'equazione (\ref {equation}) con $C$ stimata. Codice \ref {plotSoluzioneCstimata}.}}{82}{figure.caption.70}%
\contentsline {figure}{\numberline {V.11}{\ignorespaces Grafico della soluzione approssimata dell'equazione (\ref {equation}) con $C=2,11579mL/mmHg$ e $\alpha =0,97134$. Codice \ref {soluzioneCalphastimate}.}}{83}{figure.caption.71}%
\contentsline {figure}{\numberline {V.12}{\ignorespaces Grafico della soluzione dell'equazione (\ref {equation}) approssimata dopo venti cicli cardiaci con $C=2,03424mL/mmHg$ e $\alpha =0,97354$.}}{85}{figure.caption.72}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {VI.1}{\ignorespaces Distribuzione dei dati nel database.}}{94}{figure.caption.77}%
\contentsline {figure}{\numberline {VI.2}{\ignorespaces MAP: andamento del training e validation loss, early stopper, R2Score e MSE.}}{95}{figure.caption.79}%
\contentsline {figure}{\numberline {VI.3}{\ignorespaces MAP: predizioni sui dati di input.}}{96}{figure.caption.81}%
\contentsline {figure}{\numberline {VI.4}{\ignorespaces Dipendenza di MAP da $C$ sull'intervallo di training e due intervalli attigui.}}{97}{figure.caption.83}%
\contentsline {figure}{\numberline {VI.5}{\ignorespaces Dipendenza di MAP da $C$ sull'intervallo di training.}}{97}{figure.caption.84}%
\contentsline {figure}{\numberline {VI.6}{\ignorespaces Dipendenza di MAP da $C$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{98}{figure.caption.85}%
\contentsline {figure}{\numberline {VI.7}{\ignorespaces Dipendenza di MAP da $C$ sull'intervallo attiguo a destra dell'intervallo di training.}}{98}{figure.caption.86}%
\contentsline {figure}{\numberline {VI.8}{\ignorespaces Dipendenza di MAP da $R1$ sull'intervallo di training e due intervalli attigui.}}{99}{figure.caption.88}%
\contentsline {figure}{\numberline {VI.9}{\ignorespaces Dipendenza di MAP da $R1$ sull'intervallo di training.}}{99}{figure.caption.89}%
\contentsline {figure}{\numberline {VI.10}{\ignorespaces Dipendenza di MAP da $R1$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{100}{figure.caption.90}%
\contentsline {figure}{\numberline {VI.11}{\ignorespaces Dipendenza di MAP da $R1$ sull'intervallo attiguo a destra dell'intervallo di training.}}{100}{figure.caption.91}%
\contentsline {figure}{\numberline {VI.12}{\ignorespaces Dipendenza di MAP da $R2$ sull'intervallo di training e due intervalli attigui.}}{101}{figure.caption.93}%
\contentsline {figure}{\numberline {VI.13}{\ignorespaces Dipendenza di MAP da $R2$ sull'intervallo di training.}}{101}{figure.caption.94}%
\contentsline {figure}{\numberline {VI.14}{\ignorespaces Dipendenza di MAP da $R2$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{102}{figure.caption.95}%
\contentsline {figure}{\numberline {VI.15}{\ignorespaces Dipendenza di MAP da $R2$ sull'intervallo attiguo a destra dell'intervallo di training.}}{102}{figure.caption.96}%
\contentsline {figure}{\numberline {VI.16}{\ignorespaces DBP: andamento del training e validation loss, early stopper, R2Score e MSE.}}{103}{figure.caption.98}%
\contentsline {figure}{\numberline {VI.17}{\ignorespaces DBP: predizioni sui dati di input.}}{103}{figure.caption.100}%
\contentsline {figure}{\numberline {VI.18}{\ignorespaces Dipendenza di DBP da $C$ sull'intervallo di training e due intervalli attigui.}}{104}{figure.caption.102}%
\contentsline {figure}{\numberline {VI.19}{\ignorespaces Dipendenza di DBP da $C$ sull'intervallo di training.}}{104}{figure.caption.103}%
\contentsline {figure}{\numberline {VI.20}{\ignorespaces Dipendenza di DBP da $C$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{105}{figure.caption.104}%
\contentsline {figure}{\numberline {VI.21}{\ignorespaces Dipendenza di DBP da $C$ sull'intervallo attiguo a destra dell'intervallo di training.}}{105}{figure.caption.105}%
\contentsline {figure}{\numberline {VI.22}{\ignorespaces Dipendenza di DBP da $R_1$ sull'intervallo di training e due intervalli attigui.}}{106}{figure.caption.107}%
\contentsline {figure}{\numberline {VI.23}{\ignorespaces Dipendenza di DBP da $R_1$ sull'intervallo di training.}}{106}{figure.caption.108}%
\contentsline {figure}{\numberline {VI.24}{\ignorespaces Dipendenza di DBP da $R_1$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{107}{figure.caption.109}%
\contentsline {figure}{\numberline {VI.25}{\ignorespaces Dipendenza di DBP da $R_1$ sull'intervallo attiguo a destra dell'intervallo di training.}}{107}{figure.caption.110}%
\contentsline {figure}{\numberline {VI.26}{\ignorespaces Dipendenza di DBP da $R_2$ sull'intervallo di training e due intervalli attigui.}}{108}{figure.caption.112}%
\contentsline {figure}{\numberline {VI.27}{\ignorespaces Dipendenza di DBP da $R_2$ sull'intervallo di training.}}{108}{figure.caption.113}%
\contentsline {figure}{\numberline {VI.28}{\ignorespaces Dipendenza di DBP da $R_2$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{109}{figure.caption.114}%
\contentsline {figure}{\numberline {VI.29}{\ignorespaces Dipendenza di DBP da $R_2$ sull'intervallo attiguo a destra dell'intervallo di training.}}{109}{figure.caption.115}%
\contentsline {figure}{\numberline {VI.30}{\ignorespaces PP: andamento del training e validation loss, early stopper, R2Score e MSE.}}{110}{figure.caption.117}%
\contentsline {figure}{\numberline {VI.31}{\ignorespaces PP: predizioni sui dati di input.}}{110}{figure.caption.119}%
\contentsline {figure}{\numberline {VI.32}{\ignorespaces Dipendenza di PP da $C$ sull'intervallo di training e due intervalli attigui.}}{111}{figure.caption.121}%
\contentsline {figure}{\numberline {VI.33}{\ignorespaces Dipendenza di PP da $C$ sull'intervallo di training.}}{111}{figure.caption.122}%
\contentsline {figure}{\numberline {VI.34}{\ignorespaces Dipendenza di PP da $C$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{112}{figure.caption.123}%
\contentsline {figure}{\numberline {VI.35}{\ignorespaces Dipendenza di PP da $C$ sull'intervallo attiguo a destra dell'intervallo di training.}}{112}{figure.caption.124}%
\contentsline {figure}{\numberline {VI.36}{\ignorespaces Dipendenza di PP da $R1$ sull'intervallo di training e due intervalli attigui.}}{113}{figure.caption.126}%
\contentsline {figure}{\numberline {VI.37}{\ignorespaces Dipendenza di PP da $R1$ sull'intervallo di training.}}{113}{figure.caption.127}%
\contentsline {figure}{\numberline {VI.38}{\ignorespaces Dipendenza di PP da $R1$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{114}{figure.caption.128}%
\contentsline {figure}{\numberline {VI.39}{\ignorespaces Dipendenza di PP da $R1$ sull'intervallo attiguo a destra dell'intervallo di training.}}{114}{figure.caption.129}%
\contentsline {figure}{\numberline {VI.40}{\ignorespaces Dipendenza di PP da $R2$ sull'intervallo di training e due intervalli attigui.}}{115}{figure.caption.131}%
\contentsline {figure}{\numberline {VI.41}{\ignorespaces Dipendenza di PP da $R2$ sull'intervallo di training.}}{115}{figure.caption.132}%
\contentsline {figure}{\numberline {VI.42}{\ignorespaces Dipendenza di PP da $R2$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{116}{figure.caption.133}%
\contentsline {figure}{\numberline {VI.43}{\ignorespaces Dipendenza di PP da $R2$ sull'intervallo attiguo a destra dell'intervallo di training.}}{116}{figure.caption.134}%
\contentsline {figure}{\numberline {VI.44}{\ignorespaces SBP: andamento del training e validation loss, early stopper, R2Score e MSE.}}{117}{figure.caption.136}%
\contentsline {figure}{\numberline {VI.45}{\ignorespaces SBP: predizioni sui dati di input.}}{117}{figure.caption.138}%
\contentsline {figure}{\numberline {VI.46}{\ignorespaces Dipendenza di SBP da $C$ sull'intervallo di training e due intervalli attigui.}}{118}{figure.caption.140}%
\contentsline {figure}{\numberline {VI.47}{\ignorespaces Dipendenza di SBP da $C$ sull'intervallo di training.}}{118}{figure.caption.141}%
\contentsline {figure}{\numberline {VI.48}{\ignorespaces Dipendenza di SBP da $C$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{119}{figure.caption.142}%
\contentsline {figure}{\numberline {VI.49}{\ignorespaces Dipendenza di SBP da $C$ sull'intervallo attiguo a destra dell'intervallo di training.}}{119}{figure.caption.143}%
\contentsline {figure}{\numberline {VI.50}{\ignorespaces Dipendenza di SBP da $R1$ sull'intervallo di training e due intervalli attigui.}}{120}{figure.caption.145}%
\contentsline {figure}{\numberline {VI.51}{\ignorespaces Dipendenza di SBP da $R1$ sull'intervallo di training.}}{120}{figure.caption.146}%
\contentsline {figure}{\numberline {VI.52}{\ignorespaces Dipendenza di SBP da $R1$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{121}{figure.caption.147}%
\contentsline {figure}{\numberline {VI.53}{\ignorespaces Dipendenza di SBP da $R1$ sull'intervallo attiguo a destra dell'intervallo di training.}}{121}{figure.caption.148}%
\contentsline {figure}{\numberline {VI.54}{\ignorespaces Dipendenza di SBP da $R2$ sull'intervallo di training e due intervalli attigui.}}{122}{figure.caption.150}%
\contentsline {figure}{\numberline {VI.55}{\ignorespaces Dipendenza di SBP da $R2$ sull'intervallo di training.}}{122}{figure.caption.151}%
\contentsline {figure}{\numberline {VI.56}{\ignorespaces Dipendenza di SBP da $R2$ sull'intervallo attiguo a sinistra dell'intervallo di training.}}{123}{figure.caption.152}%
\contentsline {figure}{\numberline {VI.57}{\ignorespaces Dipendenza di SBP da $R2$ sull'intervallo attiguo a destra dell'intervallo di training.}}{123}{figure.caption.153}%
\addvspace {10\p@ }
