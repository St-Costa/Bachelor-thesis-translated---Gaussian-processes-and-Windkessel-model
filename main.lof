\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {II.1}{\ignorespaces Distribution function and probability density of a standard normal distribution \blx@tocontentsinit {0}\cite {wikiNormalDistribution}.}}{8}{figure.caption.7}%
\contentsline {figure}{\numberline {II.2}{\ignorespaces Point cloud generated by a bivariate Gaussian distribution with uncorrelated components \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{11}{figure.caption.8}%
\contentsline {figure}{\numberline {II.3}{\ignorespaces Point cloud generated by a bivariate Gaussian distribution with correlated components \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{12}{figure.caption.9}%
\contentsline {figure}{\numberline {II.4}{\ignorespaces Point cloud generated by a bivariate Gaussian distribution with strongly correlated components \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{13}{figure.caption.10}%
\contentsline {figure}{\numberline {II.5}{\ignorespaces Two different visual approaches to correlation of components of multivariate Gaussian vectors: $n=2$ \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{14}{figure.caption.11}%
\contentsline {figure}{\numberline {II.6}{\ignorespaces Visualization using segments of the correlation of components of multivariate Gaussian vectors: $n=5$ \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{15}{figure.caption.12}%
\contentsline {figure}{\numberline {II.7}{\ignorespaces Weak and strong correlation of components of multivariate Gaussian vectors visualized by segments \blx@tocontentsinit {0}\cite {damianou_gaussian_2016}.}}{16}{figure.caption.13}%
\contentsline {figure}{\numberline {II.8}{\ignorespaces Visualization by segments of the correlation of components of multivariate Gaussian vectors: $n=50$ \blx@tocontentsinit {0}\cite {wilkinson_introduction_2020}.}}{16}{figure.caption.14}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {III.1}{\ignorespaces Nonlinear regression \blx@tocontentsinit {0}\cite {turner_gaussian_2016}.}}{18}{figure.caption.15}%
\contentsline {figure}{\numberline {III.2}{\ignorespaces Nonlinear regression with Gaussian processes \blx@tocontentsinit {0}\cite {turner_gaussian_2016}.}}{19}{figure.caption.16}%
\contentsline {figure}{\numberline {III.3}{\ignorespaces Four vectors generated by a Gaussian process defined as in the example \ref {esempioProcessoGaussiano}. Code \ref {Example}.}}{23}{figure.caption.17}%
\contentsline {figure}{\numberline {III.4}{\ignorespaces Graph of $k(x,x')$ linear kernel, $\sigma _b^2=1$, $\sigma _v^2=1$, $c=-1$ and $x'=1$. Code \ref {linear kernel code}}}{25}{figure.caption.18}%
\contentsline {figure}{\numberline {III.5}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the linear kernel and $\sigma _b^2=0$, $\sigma _v^2=1$, $c=0$. Code \ref {linear sample}.}}{26}{figure.caption.19}%
\contentsline {figure}{\numberline {III.6}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the linear kernel and $\sigma _b^2=0$, $\sigma _v^2=1$, parameter $c$ is varied. Code \ref {Linear - c}.}}{27}{figure.caption.20}%
\contentsline {figure}{\numberline {III.7}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the linear kernel and $\sigma _v^2=1$, $c=0$, parameter $\sigma _b^2$ is varied. Code \ref {Linear - sigmab}.}}{27}{figure.caption.21}%
\contentsline {figure}{\numberline {III.8}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the linear kernel and $\sigma _b^2=0$, $c=0$, parameter $\sigma _v^2$ is varied. Code \ref {Linear - sigmav}.}}{28}{figure.caption.22}%
\contentsline {figure}{\numberline {III.9}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(m,k)$ where $m(x)=x^3$ e $k(x,x')$ is the linear kernel, $\sigma _b^2=1$, $\sigma _v^2=10$, $c=0$. Code \ref {linear cubedmean}.}}{28}{figure.caption.23}%
\contentsline {figure}{\numberline {III.10}{\ignorespaces Graph of $k(x,x')$ squared-exponential kernel, $\sigma ^2=1$ and $l^2=1$. Code \ref {squared-exponential}.}}{29}{figure.caption.24}%
\contentsline {figure}{\numberline {III.11}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the squared-exponential kernel and $l^2=1$, $\sigma ^2=1$. Code \ref {RBF sample}.}}{30}{figure.caption.25}%
\contentsline {figure}{\numberline {III.12}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the squared-exponential kernel and $l^2=1$, parameter $\sigma ^2$ is varied. Code \ref {RBF - sigma}.}}{31}{figure.caption.26}%
\contentsline {figure}{\numberline {III.13}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the squared-exponential kernel and $\sigma ^2=1$, parameter $l^2$ is varied. Code \ref {codice9}.}}{31}{figure.caption.27}%
\contentsline {figure}{\numberline {III.14}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(m,k)$ where $m(x)=x^3$ e $k(x,x')$ is the squared-exponential kernel, $\sigma ^2=7$ and $l=0.3$. Code \ref {codice10}.}}{32}{figure.caption.28}%
\contentsline {figure}{\numberline {III.15}{\ignorespaces Graph of $k(x,x')$ periodic kernel, $\sigma ^2=1$, $l^2=1$, $p=2$. Code \ref {periodic Kernel}.}}{33}{figure.caption.29}%
\contentsline {figure}{\numberline {III.16}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the periodic kernel and $\sigma ^2=1$, $l^2=2$, $p=1$. Code \ref {periodic sample}.}}{34}{figure.caption.30}%
\contentsline {figure}{\numberline {III.17}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the periodic kernel and $l^2=1, p=1$, parameter $\sigma ^2$ is varied. Code \ref {Periodic sigma}.}}{34}{figure.caption.31}%
\contentsline {figure}{\numberline {III.18}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the periodic kernel and $\sigma ^2=1$, $l^2=1$ and parameter $p$ is varied. Code \ref {periodic p}.}}{35}{figure.caption.32}%
\contentsline {figure}{\numberline {III.19}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the periodic kernel and $\sigma ^2=1$, $p=1$ and parameter $l^2$ is varied. Code \ref {periodic l}.}}{35}{figure.caption.33}%
\contentsline {figure}{\numberline {III.20}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(m,k)$ where $m(x)=x^3$ and $k(x,x')$ the periodic kernel, $\sigma ^2=8$, $l^2=1$ and $p=0.5$. Code \ref {priodic cubedmean}.}}{36}{figure.caption.34}%
\contentsline {figure}{\numberline {III.21}{\ignorespaces Graph of function with distribution $f\sim \mathcal {GP}(0,k)$ with $k(x,x')$ the squared-exponential kernel in two dimensions in which $M=\text {diag}(1,3)^{-2}$. The function tends to change faster along the $x_1$ direction than along the $x_2$ direction. \blx@tocontentsinit {0}\cite {murphy_machine_2012}}}{37}{figure.caption.35}%
\contentsline {figure}{\numberline {III.22}{\ignorespaces Graph of $k(x,x')$ squared-exponential kernel summed to periodic kernel. $\sigma ^2=1$, $l=2$, $p=1$. Code \ref {RBF + periodic kernel}.}}{39}{figure.caption.37}%
\contentsline {figure}{\numberline {III.23}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the squared-exponential summed to periodic kernel and $l^2=1.1$, $\sigma ^2=1$, $p=1.1$. Code \ref {RBF + periodic sample}.}}{39}{figure.caption.38}%
\contentsline {figure}{\numberline {III.24}{\ignorespaces Graph of $k(x,x')$ linear kernel multiplied by linear kernel. $\sigma _b^2=0$, $\sigma _v^2=1$, $c=0$, $x'=1$. Code \ref {linear x linear}.}}{40}{figure.caption.40}%
\contentsline {figure}{\numberline {III.25}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(x,x')$ is the linear kernel multiplied by linear kernel and $\sigma _b^2=0$, $\sigma _v^2=1$, $c=0$. Code \ref {linear x linear sample}.}}{40}{figure.caption.41}%
\contentsline {figure}{\numberline {III.26}{\ignorespaces 545 observations of monthly averages of the atmospheric concentration of $CO_2$ between 1958 and 2003, the $95\%$ confidence region for a 20-year Gaussian process regression model in the future is also shown. \blx@tocontentsinit {0}\cite {rasmussen_gaussian_2006}}}{41}{figure.caption.43}%
\contentsline {figure}{\numberline {III.27}{\ignorespaces Comparison of the prediction of $CO_2$ concentration with actual data until May 2022.}}{42}{figure.caption.44}%
\contentsline {figure}{\numberline {III.28}{\ignorespaces Comparison of the prediction of $CO_2$ concentration with actual data up to 1995 to May 2022.}}{43}{figure.caption.45}%
\contentsline {figure}{\numberline {III.29}{\ignorespaces Graphical explanation of how a priori knowledge is incorporated \blx@tocontentsinit {0}\cite {gortler_visual_2019}.}}{44}{figure.caption.46}%
\contentsline {figure}{\numberline {III.30}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ dove $k(\cdot ,\cdot )$ is the squared-exponential kernel; the Gaussian process was conditioned to interpolate six points. Shown in red is the function from which the points to be interpolated were chosen, in blue the mean of the conditioned Gaussian process, as dashed lines some samples of the Gaussian process. Code \ref {interpolation code}.}}{46}{figure.caption.47}%
\contentsline {figure}{\numberline {III.31}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(\cdot ,\cdot )$ is the squared-exponential kernel; the Gaussian process was conditioned to interpolate six points. Shown in blue is the 95\% confidence region, in blue the mean of the conditional Gaussian process, and as dashed lines some samples. Code \ref {interpolation confidence region code}.}}{47}{figure.caption.48}%
\contentsline {figure}{\numberline {III.32}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(\cdot ,\cdot )$ is the squared-exponential kernel; the Gaussian process was conditioned to predict a function from its noisy observations. Shown in red is the function to be predicted, in red the observed points of the function with bars representing the noise, in blue the average of the conditioned Gaussian process, as dashed lines some samples of the Gaussian process. Code \ref {Noise code}.}}{48}{figure.caption.49}%
\contentsline {figure}{\numberline {III.33}{\ignorespaces Graph of functions with distribution $f\sim \mathcal {GP}(\bm {0},k)$ where $k(\cdot ,\cdot )$ is the squared-exponential kernel; the Gaussian process was conditioned to predict a function from its noisy observations. Shown in blue is the 95\% confidence region, in red with error bars the observations, in blue the mean of the conditioned Gaussian process, and as dashed lines some samples. Code \ref {Noise confidence region code}.}}{49}{figure.caption.50}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {IV.1}{\ignorespaces Example of overfitting. Data (approximately linear) are approximated by a linear function and a polynomial function. Although the polynomial function provides an almost perfect fit, the linear function can be expected to generalise the data better. \blx@tocontentsinit {0}\cite {wiki:overfitting}}}{53}{figure.caption.51}%
\contentsline {figure}{\numberline {IV.2}{\ignorespaces Overfitting in supervised learning. The training error (error on training set) is shown in blue, the validation error (error on validation set) in red, both as a function of the number of training cycles. \blx@tocontentsinit {0}\cite {wiki:overfitting}}}{54}{figure.caption.52}%
\contentsline {figure}{\numberline {IV.3}{\ignorespaces Illustration of the gradient method on level sets, the learning rate is updated at each iteration.\blx@tocontentsinit {0}\cite {wiki:gradientDescend}}}{63}{figure.caption.53}%
\contentsline {figure}{\numberline {IV.4}{\ignorespaces Method of moments applied to the stochastic gradient method. \blx@tocontentsinit {0}\cite {ruder_2022}}}{65}{figure.caption.54}%
\contentsline {figure}{\numberline {IV.5}{\ignorespaces Comparison of methods for minimising a cost function in a neural network. \blx@tocontentsinit {0}\cite {kingma_adam_2017}}}{68}{figure.caption.55}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {V.1}{\ignorespaces Systemic pressure and aortic flow measured in a patient. Code \ref {datiReali}.}}{70}{figure.caption.56}%
\contentsline {figure}{\numberline {V.2}{\ignorespaces Real pressure graph and simple model output. Code \ref {modelloSemplice}.}}{71}{figure.caption.57}%
\contentsline {figure}{\numberline {V.3}{\ignorespaces Anatomy of the human heart \blx@tocontentsinit {0}\cite {wiki:cicloCardiaco}.}}{72}{figure.caption.58}%
\contentsline {figure}{\numberline {V.4}{\ignorespaces Diagrams summarizing the systole and diastole of a human heart \blx@tocontentsinit {0}\cite {wiki:cicloCardiaco}.}}{73}{figure.caption.59}%
\contentsline {figure}{\numberline {V.5}{\ignorespaces Example of Wiggers diagram \blx@tocontentsinit {0}\cite {wiki:DiagrammaWiggers}.}}{75}{figure.caption.61}%
\contentsline {figure}{\numberline {V.6}{\ignorespaces Illustration of the analogy on the windkessel effect \blx@tocontentsinit {0}\cite {wiki:WindkesselEffect}.}}{77}{figure.caption.66}%
\contentsline {figure}{\numberline {V.7}{\ignorespaces Illustration of the windkessel effect \blx@tocontentsinit {0}\cite {AaronsonPhilipI.PhilipIrving2020Tcsa}.}}{78}{figure.caption.67}%
\contentsline {figure}{\numberline {V.8}{\ignorespaces Circuit form of the two-element Windkessel model.}}{81}{figure.caption.68}%
\contentsline {figure}{\numberline {V.9}{\ignorespaces Graph of $f_C$. Code \ref {plotfC-code}.}}{82}{figure.caption.69}%
\contentsline {figure}{\numberline {V.10}{\ignorespaces Graph of the approximate solution of the equation (\ref {equation}) with $C$ estimated. Code \ref {plotSoluzioneCstimata}.}}{82}{figure.caption.70}%
\contentsline {figure}{\numberline {V.11}{\ignorespaces Graph of the approximate solution of the equation (\ref {equation}) with $C=2,11579mL/mmHg$ and $\alpha =0,97134$. Code \ref {soluzioneCalphastimate}.}}{83}{figure.caption.71}%
\contentsline {figure}{\numberline {V.12}{\ignorespaces Graph of the solution of the equation (\ref {equation}) approximated after twenty cardiac cycles with $C=2,03424mL/mmHg$ e $\alpha =0,97354$.}}{85}{figure.caption.72}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {VI.1}{\ignorespaces Distribution of data in the database.}}{94}{figure.caption.77}%
\contentsline {figure}{\numberline {VI.2}{\ignorespaces MAP: progress of training and validation loss, early stopper, R2Score e MSE.}}{95}{figure.caption.79}%
\contentsline {figure}{\numberline {VI.3}{\ignorespaces MAP: predictions about the input data.}}{96}{figure.caption.81}%
\contentsline {figure}{\numberline {VI.4}{\ignorespaces Dependence of MAP on $C$ on the training interval and two adjacent intervals.}}{97}{figure.caption.83}%
\contentsline {figure}{\numberline {VI.5}{\ignorespaces Dependence of MAP on $C$ over the training interval.}}{97}{figure.caption.84}%
\contentsline {figure}{\numberline {VI.6}{\ignorespaces Dependence of MAP on $C$ on the adjacent interval to the left of the training interval.}}{98}{figure.caption.85}%
\contentsline {figure}{\numberline {VI.7}{\ignorespaces Dependence of MAP on $C$ on the adjacent interval to the right of the training interval.}}{98}{figure.caption.86}%
\contentsline {figure}{\numberline {VI.8}{\ignorespaces Dependence of MAP on $R1$ on the training interval and two adjacent intervals.}}{99}{figure.caption.88}%
\contentsline {figure}{\numberline {VI.9}{\ignorespaces Dependence of MAP on $R1$ over the training interval.}}{99}{figure.caption.89}%
\contentsline {figure}{\numberline {VI.10}{\ignorespaces Dependence of MAP on $R1$ on the adjacent interval to the left of the training interval.}}{100}{figure.caption.90}%
\contentsline {figure}{\numberline {VI.11}{\ignorespaces Dependence of MAP on $R1$ on the adjacent interval to the right of the training interval.}}{100}{figure.caption.91}%
\contentsline {figure}{\numberline {VI.12}{\ignorespaces Dependence of MAP on $R2$ over the training interval and two adjacent intervals.}}{101}{figure.caption.93}%
\contentsline {figure}{\numberline {VI.13}{\ignorespaces Dependence of MAP on $R2$ over the training interval.}}{101}{figure.caption.94}%
\contentsline {figure}{\numberline {VI.14}{\ignorespaces Dependence of MAP on $R2$ on the adjoint interval to the left of the training interval.}}{102}{figure.caption.95}%
\contentsline {figure}{\numberline {VI.15}{\ignorespaces Dependence of MAP on $R2$ on the adjacent interval to the right of the training interval.}}{102}{figure.caption.96}%
\contentsline {figure}{\numberline {VI.16}{\ignorespaces DBP: progress of training and validation loss, early stopper, R2Score e MSE.}}{103}{figure.caption.98}%
\contentsline {figure}{\numberline {VI.17}{\ignorespaces DBP: input data predictions.}}{103}{figure.caption.100}%
\contentsline {figure}{\numberline {VI.18}{\ignorespaces Dependence of DBP on $C$ on the training interval and two adjacent intervals.}}{104}{figure.caption.102}%
\contentsline {figure}{\numberline {VI.19}{\ignorespaces Dependence of DBP on $C$ over the training interval.}}{104}{figure.caption.103}%
\contentsline {figure}{\numberline {VI.20}{\ignorespaces Dependence of DBP on $C$ on the adjacent interval to the left of the training interval.}}{105}{figure.caption.104}%
\contentsline {figure}{\numberline {VI.21}{\ignorespaces Dependence of DBP on $C$ on the adjacent interval to the right of the training interval.}}{105}{figure.caption.105}%
\contentsline {figure}{\numberline {VI.22}{\ignorespaces Dependence of DBP on $R_1$ on the training interval and two adjacent intervals.}}{106}{figure.caption.107}%
\contentsline {figure}{\numberline {VI.23}{\ignorespaces Dependence of DBP on $R_1$ over the training interval.}}{106}{figure.caption.108}%
\contentsline {figure}{\numberline {VI.24}{\ignorespaces Dependence of DBP on $R_1$ on the adjoint interval to the left of the training interval.}}{107}{figure.caption.109}%
\contentsline {figure}{\numberline {VI.25}{\ignorespaces Dependence of DBP on $R_1$ on the adjoint interval to the right of the training interval.}}{107}{figure.caption.110}%
\contentsline {figure}{\numberline {VI.26}{\ignorespaces Dependence of DBP on $R_2$ on the training interval and two adjacent intervals.}}{108}{figure.caption.112}%
\contentsline {figure}{\numberline {VI.27}{\ignorespaces Dependence of DBP on $R_2$ over the training interval.}}{108}{figure.caption.113}%
\contentsline {figure}{\numberline {VI.28}{\ignorespaces Dependence of DBP on $R_2$ on the adjoint interval to the left of the training interval.}}{109}{figure.caption.114}%
\contentsline {figure}{\numberline {VI.29}{\ignorespaces Dependence of DBP on $R_2$ on the adjoint interval to the right of the training interval.}}{109}{figure.caption.115}%
\contentsline {figure}{\numberline {VI.30}{\ignorespaces PP: progress of training and validation loss, early stopper, R2Score and MSE.}}{110}{figure.caption.117}%
\contentsline {figure}{\numberline {VI.31}{\ignorespaces PP: predictions about the input data.}}{110}{figure.caption.119}%
\contentsline {figure}{\numberline {VI.32}{\ignorespaces Dependence of PP on $C$ on the training interval and two adjacent intervals.}}{111}{figure.caption.121}%
\contentsline {figure}{\numberline {VI.33}{\ignorespaces Dependence of PP on $C$ over the training interval.}}{111}{figure.caption.122}%
\contentsline {figure}{\numberline {VI.34}{\ignorespaces Dependence of PP on $C$ on the adjacent interval to the left of the training interval.}}{112}{figure.caption.123}%
\contentsline {figure}{\numberline {VI.35}{\ignorespaces Dependence of PP on $C$ on the adjacent interval to the right of the training interval.}}{112}{figure.caption.124}%
\contentsline {figure}{\numberline {VI.36}{\ignorespaces Dependence of PP on $R1$ on the training interval and two adjacent intervals.}}{113}{figure.caption.126}%
\contentsline {figure}{\numberline {VI.37}{\ignorespaces Dependence of PP on $R1$ over the training interval.}}{113}{figure.caption.127}%
\contentsline {figure}{\numberline {VI.38}{\ignorespaces Dependence of PP on $R1$ on the adjoint interval to the left of the training interval.}}{114}{figure.caption.128}%
\contentsline {figure}{\numberline {VI.39}{\ignorespaces Dependence of PP on $R1$ on the adjacent interval to the right of the training interval.}}{114}{figure.caption.129}%
\contentsline {figure}{\numberline {VI.40}{\ignorespaces Dependence of PP on $R2$ on the training interval and two adjacent intervals.}}{115}{figure.caption.131}%
\contentsline {figure}{\numberline {VI.41}{\ignorespaces Dependence of PP on $R2$ over the training interval.}}{115}{figure.caption.132}%
\contentsline {figure}{\numberline {VI.42}{\ignorespaces Dependence of PP on $R2$ on the adjacent interval to the left of the training interval.}}{116}{figure.caption.133}%
\contentsline {figure}{\numberline {VI.43}{\ignorespaces Dependence of PP on $R2$ on the adjoint interval to the right of the training interval.}}{116}{figure.caption.134}%
\contentsline {figure}{\numberline {VI.44}{\ignorespaces SBP: andamento del training e validation loss, early stopper, R2Score e MSE.}}{117}{figure.caption.136}%
\contentsline {figure}{\numberline {VI.45}{\ignorespaces SBP: predictions about the input data.}}{117}{figure.caption.138}%
\contentsline {figure}{\numberline {VI.46}{\ignorespaces Dependence of SBP on $C$ on the training interval and two adjacent intervals.}}{118}{figure.caption.140}%
\contentsline {figure}{\numberline {VI.47}{\ignorespaces Dependence of SBP on $C$ over the training interval.}}{118}{figure.caption.141}%
\contentsline {figure}{\numberline {VI.48}{\ignorespaces Dependence of SBP on $C$ on the adjacent interval to the left of the training interval.}}{119}{figure.caption.142}%
\contentsline {figure}{\numberline {VI.49}{\ignorespaces Dependence of SBP on $C$ on the adjacent interval to the right of the training interval.}}{119}{figure.caption.143}%
\contentsline {figure}{\numberline {VI.50}{\ignorespaces Dependence of SBP on $R1$ on the training interval and two adjacent intervals.}}{120}{figure.caption.145}%
\contentsline {figure}{\numberline {VI.51}{\ignorespaces Dependence of SBP on $R1$ over the training interval.}}{120}{figure.caption.146}%
\contentsline {figure}{\numberline {VI.52}{\ignorespaces Dependence of SBP on $R1$ on the adjacent interval to the left of the training interval.}}{121}{figure.caption.147}%
\contentsline {figure}{\numberline {VI.53}{\ignorespaces Dependence of SBP on $R1$ on the adjacent interval to the right of the training interval.}}{121}{figure.caption.148}%
\contentsline {figure}{\numberline {VI.54}{\ignorespaces Dependence of SBP on $R2$ on the training interval and two adjacent intervals.}}{122}{figure.caption.150}%
\contentsline {figure}{\numberline {VI.55}{\ignorespaces Dependence of SBP on $R2$ over the training interval.}}{122}{figure.caption.151}%
\contentsline {figure}{\numberline {VI.56}{\ignorespaces Dependence of SBP on $R2$ on the adjacent interval to the left of the training interval.}}{123}{figure.caption.152}%
\contentsline {figure}{\numberline {VI.57}{\ignorespaces Dependence of SBP on $R2$ on the adjacent interval to the right of the training interval.}}{123}{figure.caption.153}%
\addvspace {10\p@ }
